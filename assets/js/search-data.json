{
  
    
        "post0": {
            "title": "I Finally Understood Gradient Descent: And you can too",
            "content": "The gradient descent algorithm is one of the key ingredients for the training of neural networks, but it&#39;s also probably the most difficult concept to grasp in understanding how things really work. At least in my own experience, it&#39;s been the one thing that I&#39;ve struggled to gain a deep understanding of. After juggling between a plethora of materials on the subject, I got my own eureka moment and the pieces fell into place. Like me you may also be self-taught through online courses and may have done a couple of projects but you still feel that twinge of half-knowledge, of only vaguely understanding what is actually going on when you are training neural networks (or any other maching learning model). . In this blogpost, I would be letting you in on the intuition that I&#39;ve developed, hoping that you would build on this to develop an even better understanding of this all important concept. I also provide links to the resources that have been of great help to me. . Let&#39;s begin with the gradient . But why? Because that was the most glossed over bit for me. Many online courses would just tell you that gradient descent finds the partial derivative of the loss function with respect to the weights (ie. the gradient) and takes a step in the direction opposite to this gradient; because the gradient points in the direction of steepest ascent, taking a step in the opposite direction would mean we move in the direction of steepest descent. But they never state why, I mean the why of the why. We take a step in the direction opposite to the gradient because it points in the direction of steepest ascent but the question (for me) really is: Why is the gradient the direction of steepest ascent? . Reviewing gradients . The gradient is just a vector containing all the partial derivatives of a function. Hence the key idea here is really the concept of partial derivatives. Partial derivatives tell us how much a function would change when we keep all but one of it&#39;s input variables constant and move a slight nudge in the direction of the one variable that is not fixed. . As an aside, typical neural networks contain thousands of parameters but for simplicity and ease of visualization, we would be considering functions with two variables: $f(x,y)$. Fortunately, everything we do here would generalize nicely to any number of dimensions. . For our two-variable case, the partial derivative tells us how much the output of the function would change if we keep say the $y$ variable constant and move a slight nudge in the $x$ direction and vice versa. . Concretely . . We are at the point (2, 3) in our input space which corresponds to a particular point t in the ouput plane, that is the output of our function is t for an input of (2, 3). The partial derivative with respect to $x$ tells us how much change would result in the output if we keep $y$ fixed at 3 and move slightly in the $x$ direction. Simarly, the partial derivative with respect to $y$ measures the resulting change in output when $x$ is fixed at 2 and we move a little nudge in the $y$ direction. . Let&#39;s consider the function: $f(x,y)=x^2y$. The partial derivative with respect to $x$, $ frac{ partial f}{ partial x}$ is $2xy$, ie. we keep the $y$ as a constant and differentiate the whole term. Likewise the partial derivative with respect to $x$, $ frac{ partial f}{ partial y}$ is $x^2$. . Rembering that the gradient packs together all the partial derivative into a vector, the gradient of this function would be: $$ nabla f= begin{bmatrix} 2xy x^2 end{bmatrix}$$ . Try deriving the partial derivatives for the function: $g(x,y)=3xy^3$ . Hopefully, it shouldn&#39;t be difficult to see that the gradient of this function would be: $$ nabla g= begin{bmatrix} 3y^3 9xy^2 end{bmatrix}$$ . The problem with partial derivatives is that they only tell us how things would change if we move in only one direction. Partial derivatives are partial because neither of them tells us the full story of how our function $f(x,y)$ changes when it&#39;s inputs changes. However, we do not only want to know how things change when we move in either the $x$ or $y$ direction, we want to know how much things would change if we move in any arbitrary direction within the input space. That&#39;s exactly what directional derivatives are for. . Directional derivative . The directional derivative in a direction say $ vec{w}$ tells us how much the output of the function would change if we move a slight nudge in the direction of the vector $ vec{w}$. The directional derivative is found by taking the dot product of the gradient of the function and $ vec{w}$ ie. the direction in which we want to move. . $$ nabla_ vec{w} f= nabla f cdot vec{w}$$ . If $ vec{w}= begin{bmatrix} w_1 w_2 end{bmatrix}$, then the directional derivative, $ nabla_ vec{w} f $ is equal to $ begin{bmatrix} 2xy x^2 end{bmatrix} cdot begin{bmatrix} w_1 w_2 end{bmatrix} = 2xy(w_1) + x^2(w_2)$ . Another way to look at this is to consider that in our input plane (ie. the x,y plane), any point or direction in this plane can be thought of as a combination of movements in the $x$ and $y$ directions. . . In the image above, $ vec{w}= begin{bmatrix} 3 5 end{bmatrix}$ is combination of 3 steps in the $x$ directions and 5 steps in the $y$ direction. So intuitively, taking a step along some arbitrary direction causes a change along the x-axis as well as along the y-axis. Taking the dot product for the directional derivative sums the changes along the x and y axis. . So now we have directional derivatives which are essentially a generalization of partial derivatives to deal with any arbitrary direction in our input plane. When training neural networks, the problem we seek to solve is that: given that we are at a point say (2, 3) which corresponds to a loss (the output of our function) of t, we want to know the direction that would result in the greatest increase in our loss? Once we know this direction we take a step in the opposite direction which would lead to the greatest reduction in the loss. Notice that I&#39;m placing an intentional emphasis on the word direction. We are looking for the best direction and lucky enough we already have a tool that gives a measure of how good (or bad) a particular direction is, which as you may have guessed is the directional derivative. . With directional derivatives, one way we could solve this problem is to find the directional derivative of all possible directions in which we could move. The best direction would be the direction with the largest directional derivative. But that would be too slow to compute, think about the number of possible directions in which we could move, the list is endless. However, the idea is good, we just need a simpler way to find the direction with the maximum directional derivative. . Our objective now is to find: . $$ max_{ lvert vec{w} rvert =1} nabla f(a, b) cdot vec{w}$$ . Notice that the vectors in the equation above have a magnitude or length of 1. In a sense this ensures that we don&#39;t end up picking the wrong vector just because it has a larger magnitude than the rest and hence would maximize the dot product even though it&#39;s pointing in the wrong direction. . The directional derivative as has already been stated is found by taking the dot product of the gradient and the vector pointing in our desired direction. The dot product possesses a very nice property that would allow us to find the direction that maximizes the directional derivative without having to consider all the possible directions. The dot product measures the similarity between two vectors. It assigns a score to how much the two vectors are travelling in the same direction. Formally, the dot product between two vectors $ vec{u}$ and $ vec{v}$ is: $$ vec{u} cdot vec{v} = lvert vec{u} rvert lvert vec{v} rvert cos theta$$ . Where $ theta$ is the angle between the two vectors. . . In the illustration above, when $ vec{u}= begin{bmatrix} 0 1 end{bmatrix}$ and $ vec{v}= begin{bmatrix} 1 0 end{bmatrix}$, their dot product is 0 because there is no similarity between them, $ vec{u}$ is pointing purely in the $x$ direction (it has no $y$ component whiles $ vec{v}$ also points solely in the $y$ direction. The angle between them is $90^ circ$ (they are perpendicular) and $cos(90^ circ)=0$. When, $ vec{u}= begin{bmatrix} 0 1 end{bmatrix}$ and $ vec{v}= begin{bmatrix} 0 1 end{bmatrix}$, the dot product is maximized because they are pointing in the same direction. The dot product in this case is 1 and the angle between them is $0^ circ$ $$ vec{u} cdot vec{v} = 1 cdot 1 cdot cos(0^ circ) = 1$$ Both vectors have a length of 1. From the foregoing, it is not difficult to grasp the fact that we are driving at, which is that, for unit length vectors, the dot product is maximized when the two vectors are parallel, that is they point in the same direction. . To remind us of our objective, we want to find the vector that maximizes the directional derivative: $$ max_{ lvert vec{w} rvert =1} nabla f(a, b) cdot vec{w}$$ . The directional derivative is also a dot product and so it flows naturally from our understanding of dot products that the vector that would maximize the directional derivative and result in the greatest increase in our function is the vector that points in the same direction as the gradient which is the gradient itself. This is why the gradient is the direction of steepest ascent. Gradient descent takes a step in the opposite direction because our objective in training is to minimize, not to maximize the loss function. . Hopefully you&#39;ve gained some usefull insights from this post to help you solidify your understanding of the foundations of neural networks and other machine learning algorithms. . Resources . Partial and Directional derivatives: Khan Academy: Multivariable calculus Dot products: Khan Academy: Linear Algebra Calculus: 3Blue1Brown .",
            "url": "https://asiedubrempong.github.io/logits/jupyter/foundations/2020/06/27/gradient-ascent.html",
            "relUrl": "/jupyter/foundations/2020/06/27/gradient-ascent.html",
            "date": " • Jun 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://asiedubrempong.github.io/logits/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://asiedubrempong.github.io/logits/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://asiedubrempong.github.io/logits/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}